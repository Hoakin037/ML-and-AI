{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивная постановка задачи кластеризации довольно проста и представляет из себя наше желание сказать: \"Вот тут у меня насыпаны точки. Я вижу, что они сваливаются в какие-то кучки вместе. Было бы круто иметь возможность эти точки относить к кучкам и в случае появления новой точки на плоскости говорить, в какую кучку она падает.\" Из такой постановки видно, что пространства для фантазии получается много, и от этого возникает соответствующее множество алгоритмов решения этой задачи. Перечисленные алгоритмы ни в коем случае не описывают данное множество полностью, но являются примерами самых популярных методов решения задачи кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм К-средних, наверное, самый популярный и простой алгоритм кластеризации и очень легко представляется в виде простого псевдокода:\n",
    "\n",
    "* Выбрать количество кластеров *K*, которое нам кажется оптимальным для наших данных.\n",
    "* Высыпать случайным образом в пространство наших данных K точек (центроидов).\n",
    "* Для каждой точки нашего набора данных посчитать, к какому центроиду она ближе.\n",
    "* Переместить каждый центроид в центр выборки, которую мы отнесли к этому центроиду.\n",
    "* Повторять последние два шага фиксированное число раз, либо до тех пор пока центроиды не \"сойдутся\" (обычно это значит, что их смещение относительно предыдущего положения не превышает какого-то заранее заданного небольшого значения).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем библиотеку matplotlib для визуализации данных\n",
    "from matplotlib import pyplot as plt \n",
    "# Импортируем numpy для работы с массивами и генерации случайных чисел\n",
    "import numpy as np\n",
    "\n",
    "# Создаём массив X размером 150x2 для хранения координат 150 точек в двумерном пространстве\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "# Устанавливаем seed для воспроизводимости случайных чисел\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "# Генерируем три кластера по 50 точек каждый с разными центрами и масштабами\n",
    "# Первый кластер: центр в (0, 0), стандартное отклонение 0.3 для обеих осей\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "# Второй кластер: центр в (2, -1), стандартное отклонение 0.5 по x и 0.2 по y\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "# Третий кластер: центр в (-1, 2), стандартное отклонение 0.2 по x и 0.5 по y\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "# Создаём график размером 5x5 дюймов\n",
    "plt.figure(figsize=(5, 5))\n",
    "# Визуализируем точки как синие круги ('bo' — blue circles)\n",
    "plt.plot(X[:, 0], X[:, 1], 'bo')\n",
    "# Отображаем график\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем функцию cdist из scipy для вычисления расстояний между точками\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Устанавливаем seed для воспроизводимости случайных чисел\n",
    "np.random.seed(seed=42)\n",
    "# Генерируем три случайных центроида (K=3) с координатами, распределёнными нормально\n",
    "# Центр в (0, 0), стандартное отклонение 1.0\n",
    "centroids = np.random.normal(loc=0.0, scale=1., size=6)\n",
    "# Преобразуем массив в форму (3, 2) для трёх центроидов с координатами x и y\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "# Создаём список для хранения истории положений центроидов на каждой итерации\n",
    "cent_history = []\n",
    "# Сохраняем начальные положения центроидов\n",
    "cent_history.append(centroids)\n",
    "\n",
    "# Выполняем три итерации алгоритма K-means\n",
    "for i in range(3):\n",
    "    # Вычисляем евклидовы расстояния от каждой точки данных до каждого центроида\n",
    "    distances = cdist(X, centroids)\n",
    "    # Для каждой точки определяем ближайший центроид (индекс минимального расстояния)\n",
    "    labels = distances.argmin(axis=1)\n",
    "    \n",
    "    # Создаём копию текущих центроидов, чтобы обновить их позиции\n",
    "    centroids = centroids.copy()\n",
    "    # Обновляем координаты каждого центроида как среднее арифметическое координат точек,\n",
    "    # принадлежащих соответствующему кластеру\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "    \n",
    "    # Сохраняем новые положения центроидов в историю\n",
    "    cent_history.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем модуль hierarchy и функцию pdist из scipy для иерархической кластеризации\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Создаём массив X размером 150x2 для хранения координат 150 точек\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "# Устанавливаем seed для воспроизводимости случайных чисел\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "# Генерируем три кластера по 50 точек, как в первом коде\n",
    "# Первый кластер: центр в (0, 0), стандартное отклонение 0.3\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "# Второй кластер: центр в (2, -1), стандартное отклонение 0.5 по x и 0.2 по y\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "# Третий кластер: центр в (-1, 2), стандартное отклонение 0.2 по x и 0.5 по y\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "# Вычисляем матрицу попарных расстояний между точками (верхний треугольник)\n",
    "distance_mat = pdist(X)\n",
    "\n",
    "# Применяем агломеративную иерархическую кластеризацию с методом 'single linkage'\n",
    "# 'single' означает, что расстояние между кластерами определяется как минимальное\n",
    "# расстояние между любой парой точек из разных кластеров\n",
    "Z = hierarchy.linkage(distance_mat, 'single')\n",
    "\n",
    "# Создаём график размером 10x5 дюймов для отображения дендрограммы\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Строим дендрограмму, где цвет ветвей меняется при пороге 0.5\n",
    "dn = hierarchy.dendrogram(Z, color_threshold=0.5)\n",
    "# Отображаем дендрограмму\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача оценки качества кластеризации является более сложной по сравнению с оценкой качества классификации. Во-первых, такие оценки не должны зависеть от самих значений меток, а только от самого разбиения выборки. Во-вторых, не всегда известны истинные метки объектов, поэтому также нужны оценки, позволяющие оценить качество кластеризации, используя только неразмеченную выборку.\n",
    "\n",
    "Выделяют внешние и внутренние метрики качества. Внешние используют информацию об истинном разбиении на кластеры, в то время как внутренние метрики не используют никакой внешней информации и оценивают качество кластеризации, основываясь только на наборе данных. Оптимальное число кластеров обычно определяют с использованием внутренних метрик.\n",
    "\n",
    "Все указанные ниже метрики реализованы в sklearn.metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted Rand Index (ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предполагается, что известны истинные метки объектов. Данная мера не зависит от самих значений меток, а только от разбиения выборки на кластеры. Пусть $N$ - число объектов в выборке. Обозначим через $a$ - число пар объектов, имеющих одинаковые метки и находящихся в одном кластере, через $b$ - число пар объектов, имеющих различные метки и находящихся в разных кластерах. Тогда Rand Index это $$\\text{RI} = \\frac{2(a + b)}{n(n-1)}.$$ То есть это доля объектов, для которых эти разбиения (исходное и полученное в результате кластеризации) \"согласованы\". Rand Index (RI) выражает схожесть двух разных кластеризаций одной и той же выборки. Чтобы этот индекс давал значения близкие к нулю для случайных кластеризаций при любом $N$ и числе кластеров, необходимо нормировать его. Так определяется Adjusted Rand Index: $$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$$\n",
    "\n",
    "Эта мера симметрична, не зависит от значений и перестановок меток. Таким образом, данный индекс является мерой расстояния между различными разбиениями выборки. $\\text{ARI}$ принимает значения в диапазоне $[-1, 1]$. Отрицательные значения соответствуют \"независимым\" разбиениям на кластеры, значения, близкие к нулю, - случайным разбиениям, и положительные значения говорят о том, что два разбиения схожи (совпадают при $\\text{ARI} = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted Mutual Information (AMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная мера очень похожа на $\\text{ARI}$. Она также симетрична, не зависит от значений и перестановок меток. Определяется с использованием функции энтропии, интерпретируя разбиения выборки, как дискретные распределения (вероятность отнесения к кластеру равна доле объектов в нём). Индекс $MI$ определяется как взаимная информация для двух распределений, соответствующих разбиениям выборки на кластеры. Интуитивно, взаимная информация измеряет долю информации, общей для обоих разбиений: насколько информация об одном из них уменьшает неопределенность относительно другого.\n",
    "\n",
    "Аналогично $\\text{ARI}$ определяется индекс $\\text{AMI}$, позволяющий избавиться от роста индекса $MI$ с увеличением числа классов. Он принимает значения в диапазоне $[0, 1]$. Значения, близкие к нулю, говорят о независимости разбиений, а близкие к единице - об их схожести (совпадении при $\\text{AMI} = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Гомогенность, полнота, V-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально данные меры также определяются с использованием функций энтропии и условной энтропии, рассматривая разбиения выборки как дискретные распределения:$$h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)},$$ здесь $K$ - результат кластеризации, $C$ - истинное разбиение выборки на классы. Таким образом, $h$ измеряет, насколько каждый кластер состоит из объектов одного класса, а $c$ - насколько объекты одного класса относятся к одному кластеру. Эти меры не являются симметричными. Обе величины принимают значения в диапазоне $[0, 1]$, и большие значения соответствуют более точной кластеризации. Эти меры не являются нормализованными, как $\\text{ARI}$ или $\\text{AMI}$, и поэтому зависят от числа кластеров. Случайная кластеризация не будет давать нулевые показатели при большом числе классов и малом числе объектов. В этих случаях предпочтительнее использовать $\\text{ARI}$. Однако при числе объектов более 1000 и числе кластеров менее 10 данная проблема не так явно выражена и может быть проигнорирована.\n",
    "\n",
    "Для учёта обеих величин $h$ и $c$ одновременно вводится $V$-мера, как их среднее гармоническое: $$v = 2\\frac{hc}{h+c}.$$ Она является симметричной и показывает, насколько две кластеризации схожи между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Силуэт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от описанных выше метрик, данный коэффициент не предполагает знания истинных меток объектов, и позволяет оценить качество кластеризации, используя только саму (неразмеченную) выборку и результат кластеризации. Сначала силуэт определяется отдельно для каждого объекта. Обозначим через $a$ - среднее расстояние от данного объекта до объектов из того же кластера, через $b$ - среднее расстояние от данного объекта до объектов из ближайшего кластера (отличного от того, в котором лежит сам объект). Тогда силуэтом данного объекта называется величина: $$s = \\frac{b - a}{\\max(a, b)}.$$ Силуэтом выборки называется средняя величина силуэта объектов данной выборки. Таким образом, силуэт показывает, насколько среднее расстояние до объектов своего кластера отличается от среднего расстояния до объектов других кластеров. Данная величина лежит в диапазоне $[-1, 1]$. Значения, близкие к -1, соответствуют плохим (разрозненным) кластеризациям, значения, близкие к нулю, говорят о том, что кластеры пересекаются и накладываются друг на друга, значения, близкие к 1, соответствуют \"плотным\" четко выделенным кластерам. Таким образом, чем больше силуэт, тем более четко выделены кластеры, и они представляют собой компактные, плотно сгруппированные облака точек.\n",
    "\n",
    "С помощью силуэта можно выбирать оптимальное число кластеров $k$ (если оно заранее неизвестно) - выбирается число кластеров, максимизирующее значение силуэта. В отличие от предыдущих метрик, силуэт зависит от формы кластеров, и достигает больших значений на более выпуклых кластерах, получаемых с помощью алгоритмов, основанных на восстановлении плотности распределения.\n",
    "\n",
    "И напоследок давайте посмотрим на эти метрики для наших алгоритмов, запущенных на данных рукописных цифр MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки и модули\n",
    "from sklearn import metrics  # Для вычисления метрик качества кластеризации\n",
    "from sklearn import datasets  # Для загрузки датасета MNIST\n",
    "import pandas as pd  # Для создания таблицы с результатами\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering  # Алгоритмы кластеризации\n",
    "\n",
    "# Загружаем датасет MNIST (рукописные цифры)\n",
    "data = datasets.load_digits()\n",
    "# Разделяем данные на признаки (X) и истинные метки (y)\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Создаём список алгоритмов кластеризации для сравнения\n",
    "algorithms = []\n",
    "# K-means с 10 кластерами (так как в MNIST 10 цифр), фиксированным random_state для воспроизводимости\n",
    "algorithms.append(KMeans(n_clusters=10, random_state=1))\n",
    "# Affinity Propagation (не требует задания числа кластеров)\n",
    "algorithms.append(AffinityPropagation())\n",
    "# Спектральная кластеризация с 10 кластерами и использованием ближайших соседей для матрицы похожести\n",
    "algorithms.append(SpectralClustering(n_clusters=10, random_state=1, affinity='nearest_neighbors'))\n",
    "# Агломеративная кластеризация с 10 кластерами\n",
    "algorithms.append(AgglomerativeClustering(n_clusters=10))\n",
    "\n",
    "# Создаём список для хранения результатов метрик\n",
    "data = []\n",
    "# Для каждого алгоритма:\n",
    "for algo in algorithms:\n",
    "    # Применяем алгоритм к данным\n",
    "    algo.fit(X)\n",
    "    # Вычисляем метрики качества кластеризации\n",
    "    data.append({\n",
    "        'ARI': metrics.adjusted_rand_score(y, algo.labels_),  # Adjusted Rand Index\n",
    "        'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_),  # Adjusted Mutual Information\n",
    "        'Homogenity': metrics.homogeneity_score(y, algo.labels_),  # Гомогенность\n",
    "        'Completeness': metrics.completeness_score(y, algo.labels_),  # Полнота\n",
    "        'V-measure': metrics.v_measure_score(y, algo.labels_),  # V-мера\n",
    "        'Silhouette': metrics.silhouette_score(X, algo.labels_)  # Силуэт\n",
    "    })\n",
    "\n",
    "# Создаём DataFrame для отображения результатов\n",
    "results = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=['ARI', 'AMI', 'Homogenity', 'Completeness', 'V-measure', 'Silhouette'],\n",
    "    index=['K-means', 'Affinity', 'Spectral', 'Agglomerative']\n",
    ")\n",
    "\n",
    "# Выводим таблицу с результатами\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё один пример алгоритма кластеризации. В отличие от алгоритма К-средних, данный подход не требует заранее определять число кластеров на которое мы хотим разбить наши данные. Основная идея алгоритма заключается в том, что нам хотелось бы, чтобы наши наблюдения кластеризовались в группы на основе того, как они \"общаются\" или насколько они похожи друг на друга.\n",
    "\n",
    "Заведём для этого какую-нибудь метрику \"похожести\", определяющуюся тем, что $s(x_i, x_j) > s(x_i, x_k)$ если наблюдение $x_i$ больше похоже на наблюдение $x_j$, чем на $x_k$. Простым примером такой похожести будет отрицательный квадрат расстояния $s(x_i, x_j) = - ||x_i - x_j||^{2}$.\n",
    "\n",
    "Теперь опишем сам процесс \"общения\" для этого заведём две матрицы, инициализируемые нулями, одна из которых $r_{i,k}$ будет описывать насколько хорошо $k$-тое наблюдение подходит для того, чтобы быть \"примером для подражания\" для $i$-того наблюдения, относительно всех остальных потенциальных \"примеров\", а вторая — $a_{i,k}$ будет описывать насколько правильным было бы для $i$-того наблюдения выбрать $k$-тое в качестве такого \"примера\". Звучит немного запутанно, но чуть дальше увидим пример \"на пальцах\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Спектральная кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спектральная кластеризация объединяет несколько описанных выше подходов, чтобы получить максимальное количество профита от сложных многообразий размерности меньшей исходного пространства.\n",
    "\n",
    "Для работы этого алгоритма нам потребуется определить матрицу похожести наблюдений (adjacency matrix). Можно это сделать таким же образом, как и для Affinity Propagation выше: $A_{i, j} = - ||x_i - x_j||^{2}$. Эта матрица также описывает полный граф с вершинами в наших наблюдениях и рёбрами между каждой парой наблюдений с весом, соответствующим степени похожести этих вершин. Для нашей выше выбранной метрики и точек, лежащих на плоскости, эта штука будет интуитивной и простой — две точки более похожи, если ребро между ними короче. Теперь нам бы хотелось разделить наш получившийся граф на две части так, чтобы получившиеся точки в двух графах были в общем больше похожи на другие точки внутри получившейся \"своей\" половины графа, чем на точки в \"другой\" половине."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}